{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adverse-diploma",
   "metadata": {},
   "source": [
    "<font color='orange' size=6>Authorship, Manning Live Project, Section 2</font>  \n",
    "Mar 12, 2021  \n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "french-physics",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "outer-toronto",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from chardet.universaldetector import UniversalDetector\n",
    "import pickle\n",
    "import glob\n",
    "import chardet\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from typing import NamedTuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "returning-concern",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from project 1; for application of that code below\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-cheese",
   "metadata": {},
   "source": [
    "# Config including paths, globals, constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "looking-measurement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/bradgreenwald/projects/manning/Author_ID_Live_Project'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check current working dir and create dir for files\n",
    "cur_dir = os.getcwd()\n",
    "cur_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fossil-preliminary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/bradgreenwald/projects/manning/Author_ID_Live_Project/data/blogs'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# path to directory containing xml files with blogs\n",
    "xml_path = os.path.join(os.getcwd(), 'data', 'blogs')\n",
    "xml_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "intermediate-farming",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19320"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create list of xml_files, recalling that mac creates files in the folder\n",
    "xml_filenames = [fname for fname in os.listdir(xml_path) if fname.endswith('xml')]\n",
    "len(xml_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-optimum",
   "metadata": {},
   "source": [
    "# Helper Classes, Functions; also Hoisted Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "coated-shore",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirp = lambda x: [d for d in dir(x) if not d.startswith('_')]\n",
    "from pprint import pprint as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-quarter",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Use xml parsing from Beatiful Soup to get the content of the posts tag. Later this can be expanded to get dates\"\"\"\n",
    "def get_posts(txt1):\n",
    "    \"\"\"Return list of posts-tag text content from byte content txt1\"\"\"\n",
    "    soup = bs(txt1, 'lxml-xml')\n",
    "    ans = soup.find_all('post')\n",
    "    posts = []\n",
    "    for ch in ans:\n",
    "        posts.append(ch.text)\n",
    "    return posts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-hartford",
   "metadata": {},
   "source": [
    "# Pickled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "right-invalid",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xml files byte content in dict keyed by filename\n",
    "with open(os.path.join(os.getcwd(), 'data', 'xml_files.pickle'), 'rb') as fh:\n",
    "    xml_files = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fewer-vector",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoded text dataframe with filenames\n",
    "with open(os.path.join(cur_dir, 'data', 'df_txt.pickle'), 'rb') as fh:\n",
    "    df_txt = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "original-suspension",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txt</th>\n",
       "      <th>fnames</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8290</th>\n",
       "      <td>&lt;Blog&gt;\\r\\n\\r\\n&lt;date&gt;18,October,2003&lt;/date&gt;\\r\\n...</td>\n",
       "      <td>2129306.female.23.Student.Sagittarius.xml</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    txt  \\\n",
       "8290  <Blog>\\r\\n\\r\\n<date>18,October,2003</date>\\r\\n...   \n",
       "\n",
       "                                         fnames  \n",
       "8290  2129306.female.23.Student.Sagittarius.xml  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_txt.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "educated-happening",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned post processed posts with metadata\n",
    "with open(os.path.join(cur_dir, 'data', 'df_allclean.pickle'), 'rb') as fh:\n",
    "    df_allposts = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dress-separation",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Write a function that reads in the contents from one of the blogs files as binary data. Donâ€™t try to parse the contents yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-hardwood",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# get first file\n",
    "file1 = os.listdir(xml_path)[0]\n",
    "file1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powerful-knock",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_bytes(xml_filename):\n",
    "    \"\"\"Read in the bytes from the xml file\"\"\"\n",
    "    with open(os.path.join(xml_path, xml_filename), 'rb') as fh:\n",
    "        content = fh.read()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-banana",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "content = get_bytes(file1)\n",
    "len(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-solid",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Get all files as bytes into pickled dictionary for eacy access, keyed by filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outer-channels",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "xml_files = dict()\n",
    "for fname in xml_filenames:\n",
    "    with open(os.path.join(os.getcwd(), xml_path, fname), 'rb') as fh:\n",
    "        content = fh.read()\n",
    "        xml_files[fname] = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accompanied-heart",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(os.getcwd(), 'data', 'xml_files.pickle'), 'wb') as fh:\n",
    "    pickle.dump(xml_files, fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-shore",
   "metadata": {},
   "source": [
    "# Different Approach:  using the most common encodings, try to apply them.  No using chardet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exciting-links",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Get Encodings by Guess and Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "possible-company",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Turns out that at least one of the files that chardet returned None for encoding was decoded fine when I used ascii. I wonder how many times that would happen if we just use ascii, utf-8, windows-1252, iso-8859 (latin1)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interstate-resource",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TODO:  Refactor to use a list of 'encs_to_try' instead of hardcoding and not iterating\n",
    "def encodings_guess_and_check(encs_to_try, xml_files_dict):\n",
    "    \"\"\"Determines file counts for a set of encodings by guess and check, for dict of bytes-files keyed by filename\n",
    "    Args:\n",
    "        encs_to_try: list of encodings, e.g. sample to get most popular\n",
    "        xml_files_dict: dict keyed by filename, vals are encodings that worked (i.e. file decoded with no error)\n",
    "    Return:\n",
    "        fnames: list of filenames\n",
    "        dlist: list of strings from applying determined encoding\n",
    "        file_enc_dist: dict keyed by filename, vals are determined encoding\n",
    "        decoded_dict: dict keyed by filename, vals are the strings resulting from applying determined encoding\n",
    "    \"\"\"\n",
    "    file_enc_dict = dict()\n",
    "    decoded_dict = dict()\n",
    "    fnames = []\n",
    "    dlist = []\n",
    "    for fname, content in xml_files.items(): \n",
    "        try: \n",
    "            txt = content.decode(encoding='utf-8')\n",
    "            file_enc_dict[fname] = 'utf-8'\n",
    "            decoded_dict[fname] = txt\n",
    "            dlist.append(txt)\n",
    "            fnames.append(fname)\n",
    "        except UnicodeDecodeError:\n",
    "            try: \n",
    "                txt = content.decode(encoding='Windows-1252')\n",
    "                file_enc_dict[fname] = 'Windows-1252'\n",
    "                decoded_dict[fname] = txt\n",
    "                dlist.append(txt)\n",
    "                fnames.append(fname)\n",
    "            except UnicodeDecodeError:\n",
    "                try:\n",
    "                    txt = content.decode(encoding='ISO-8859-1')\n",
    "                    file_enc_dict[fname] = 'ISO-8859-1'\n",
    "                    decoded_dict[fname] = txt\n",
    "                    dlist.append(txt)\n",
    "                    fnames.append(fname)\n",
    "                except:\n",
    "                    count += 1\n",
    "                    print(f'{count = }')\n",
    "    return fnames, dlist, file_enc_dict, decoded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-steam",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x_dict = {\n",
    "    'txt': dlist,\n",
    "    'fnames': fnames\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-longitude",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_txt = pd.DataFrame(x_dict)\n",
    "df_txt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environmental-sport",
   "metadata": {
    "hidden": true
   },
   "source": [
    "of 19320 files, only 607 have errors when using utf-8\n",
    "- and using Windows-1252 as second tier leaves only 33.  \n",
    "- and using ISO-8859-1, it decodes them all.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "celtic-ordinary",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Pickle the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-landing",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(cur_dir, 'data', 'df_txt.pickle'), 'wb') as fh:\n",
    "    pickle.dump(df_txt, fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-wednesday",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Explore and Parse the Decoded Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "voluntary-agent",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "txt1 = df_txt.loc[0, 'txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "given-cooler",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "txt1[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "negative-thesaurus",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Use xml parsing from Beatiful Soup to get the content of the posts tag. Later this can be expanded to get dates\"\"\"\n",
    "def get_posts(txt1):\n",
    "    \"\"\"Return list of posts-tag text content from byte content txt1\"\"\"\n",
    "    soup = bs(txt1, 'lxml-xml')\n",
    "    ans = soup.find_all('post')\n",
    "    posts = []\n",
    "    for ch in ans:\n",
    "        posts.append(ch.text)\n",
    "    return posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approved-clear",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "p3 = parse_xml(dlist[3])\n",
    "len(p3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-collective",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Check posts\n",
    "for p in p3:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specified-eating",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Post processing**  \n",
    "- When the blogs are printed, they seem much better than the raw string, thus one of the issues will be **formatting chars**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-production",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Make dataframe with the parsed posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purple-while",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# change index to filename for easier access\n",
    "df_txt.set_index('fnames', inplace=True)\n",
    "df_txt.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-spell",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Class for posts to use for creating dataframe\n",
    "class BlogPost(NamedTuple):\n",
    "    auth_id: int\n",
    "    gender: str\n",
    "    age: int\n",
    "    industry: str\n",
    "    star_sign: str\n",
    "    post: str\n",
    "    \n",
    "#     def __repr__(self):\n",
    "#         return f'n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-senator",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create df for first 5 authors similar to df in part 1, using metadata\n",
    "def posts_to_df(filenames):\n",
    "    \"\"\"Create dataframe for filenames, where each row is a post and includes metadata parsed from filename\"\"\"\n",
    "    # get metadata\n",
    "    blogposts = []\n",
    "    for fname in filenames:\n",
    "        auth_id, gender, age, industry, star_sign = fname.split('.')[:-1]\n",
    "        posts = get_posts(df_txt.loc[fname]['txt'])\n",
    "        for post in posts:\n",
    "            bp = BlogPost(auth_id, gender, age, industry, star_sign, post)\n",
    "            blogposts.append(bp)\n",
    "    return blogposts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-colorado",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fname_all = xml_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominant-greenhouse",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bp_all = posts_to_df(fname_all)\n",
    "len(bp_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vanilla-chick",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_allposts = pd.DataFrame(bp_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-prize",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_allposts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ignored-jacob",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Learned this last lesson, but worth repeating:\n",
    "NamedTuples convert easily to pandas DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustained-cooking",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Review 100 randomly sampled text documents from the set. Are there any post-processing steps needed? Are there aspects to the text that should be removed from the data before using it in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organizational-oakland",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_allposts.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smart-moderator",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sample_100 = df_allposts.sample(100)\n",
    "sample_100.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educational-photographer",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "g = (row for row in sample_100.iterrows())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-fraction",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ii = iter(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-memorial",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What makes a difference to authors?  prob not the regular stuff that we do for content, like lowercase.  Uppercase could be important. \n",
    "- suggestion to remove numbers\n",
    "- suggestion to remove urllink\n",
    "- urls\n",
    "- formatting\n",
    "- punctuation may be important for authors\n",
    "- maybe things like trademark or copyright symbols\n",
    "- maybe proper names of certain sorts like Months, places, because places are the same and don't really distinguish an authors style, especially if we are doing char-wise.  Maybe word-wise. \n",
    "- maybe NER\n",
    "- quotations\n",
    "- we could remove very common words?\n",
    "- maybe parens? \n",
    "- there's other stuff that you start to notice that maybe won't help to distinguish authors.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medical-transmission",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def post_process(post):\n",
    "    post.strip()\n",
    "    re_urllink = re.compile('urlLink')\n",
    "    # how to remove nums?  i guess just remove each digit, replace with space, which will disappear? or we can \n",
    "    # normalize all spaces to single space.  or for authorship, maybe we want to Not look at spaces? \n",
    "    re_remove_nums = re.compile('\\d[a-zA-Z0-9]') # remove entire thing with the number in it\n",
    "    re_urls = re.compile('http.+')\n",
    "    re_white_space = re.compile(r'[\\n\\t\\r]+') # replace with space.  lastly convert all white space to one. \n",
    "    re_alot_white_space = re.compile(r'[\\s]+')\n",
    "    \n",
    "    post = re_urllink.sub(' ', post)\n",
    "    post = re_remove_nums.sub(' ', post)\n",
    "    post = re_urls.sub(' ', post)\n",
    "    post = re_white_space.sub(' ', post)\n",
    "    post = re_alot_white_space.sub(' ', post)\n",
    "    return post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intimate-dialogue",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "h = next(ii)\n",
    "print(len(h[1]['post']))\n",
    "print(post_process(h[1]['post']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-monaco",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "new = post_process(df_allposts.loc[0, 'post'])\n",
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acute-sandwich",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "re.sub(r'[\\s]+',' ', new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appointed-helen",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Try to apply to the post feature by row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-nirvana",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# test\n",
    "clean_posts = []\n",
    "for txt in df_allposts[:10]['post']:\n",
    "    print(post_process(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-syndrome",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_allposts['clean'] = df_allposts['post'].apply(post_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "veterinary-friday",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# looks like it worked.  gonna pickle it\n",
    "df_allposts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standing-payment",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### pickle df_cleanposts\n",
    "with open(os.path.join(cur_dir, 'data', 'df_allclean.pickle'), 'wb') as fh:\n",
    "    pickle.dump(df_allposts, fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "religious-singer",
   "metadata": {},
   "source": [
    "# Update the dataset loading code from milestone 1 to use this process of BeautifulSoup to load the data, and then your post-processing script afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-atlas",
   "metadata": {},
   "source": [
    "- I already did that as part of the process. Actually I'm not sure how we could have done the above other than if we did this, unless, again, I misunderstood the instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loving-knife",
   "metadata": {},
   "source": [
    "# Finalize the workflow by ensuring you can read your documents in, clean them up, and classify them using the rest of the procedure we used in Milestone 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surface-savannah",
   "metadata": {},
   "source": [
    "- Using the list of authors from part 1, get all their posts\n",
    "- sample 10 for each author (from the solution template)\n",
    "- create train-test split in portion of dataset (some columns)\n",
    "- create pipeline\n",
    "- apply pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "express-rover",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast auth_id and age as ints\n",
    "df_allposts['auth_id'] = df_allposts['auth_id'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "favorite-springfield",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = [3574878, 2845196, 3444474, 3445677, 828046, \n",
    "                       4284264, 3498812, 4137740, 3662461, 3363271]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-dictionary",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth10_posts = df_allposts[df_allposts['auth_id'].isin(authors)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-smart",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth10_posts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driving-liquid",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth10_posts.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surface-convertible",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = auth10_posts['clean']\n",
    "all_labels = auth10_posts['auth_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-strand",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the template - we may not need the transformer because we are using tfidf to vectorize as well as transform.\n",
    "auth_clf = Pipeline([\n",
    "    ('vect', TfidfVectorizer(ngram_range=(1, 6), analyzer='char', use_idf='false')),\n",
    "    ('clf', SGDClassifier()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "internal-swaziland",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_to_f1_scores(diff_samples, clfs):\n",
    "    \"\"\"Apply the methodology used above on different sets of sampled data\"\"\"\n",
    "    data = diff_samples['post']\n",
    "    labels = diff_samples['auth_id']\n",
    "    df_auth_train, df_auth_test, labels_train, labels_test = train_test_split(data, labels, \n",
    "                                                                          test_size=0.3, stratify=labels)\n",
    "    clfs.fit(df_auth_train, labels_train)\n",
    "    preds = clfs.predict(df_auth_test)\n",
    "    acc = np.mean(preds==labels_test)\n",
    "\n",
    "    # calculate the f1-scores; labels_test is global\n",
    "    \n",
    "    print(f'f-scores')\n",
    "    for avg in ['weighted', 'macro', 'micro']:\n",
    "        f1 = metrics.f1_score(labels_test, preds, average=avg, zero_division=0)\n",
    "        print(f'\\t{avg:8} {f1:.4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "political-vatican",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_to_f1_scores(auth10_posts, auth_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tracked-bunny",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "included-angel",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
