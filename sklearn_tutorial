{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "proved-sussex",
   "metadata": {},
   "source": [
    "<font color='orange' size=6>Standard Text Mining Pipeline</font>  \n",
    "Working through a tutorial in Sci-kit Learn, as recommended from **LiveProject on Authorship analysis**\n",
    "<hr>\n",
    "Mar 7, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-agency",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Scratch and Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-peace",
   "metadata": {
    "hidden": true
   },
   "source": [
    "~~there~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "touched-democrat",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eastern-regular",
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at /Users/bradgreenwald/miniconda3/envs/nlp:\r\n",
      "#\r\n",
      "# Name                    Version                   Build  Channel\r\n",
      "appnope                   0.1.2           py38hecd8cb5_1001    defaults\r\n",
      "argon2-cffi               20.1.0           py38h9ed2024_1    defaults\r\n",
      "async_generator           1.10               pyhd3eb1b0_0    defaults\r\n",
      "attrs                     20.3.0             pyhd3eb1b0_0    defaults\r\n",
      "backcall                  0.2.0              pyhd3eb1b0_0    defaults\r\n",
      "beautifulsoup4            4.9.3              pyha847dfd_0    defaults\r\n",
      "blas                      1.0                         mkl    defaults\r\n",
      "bleach                    3.3.0              pyhd3eb1b0_0    defaults\r\n",
      "brotlipy                  0.7.0           py38h9ed2024_1003    defaults\r\n",
      "ca-certificates           2021.1.19            hecd8cb5_0    defaults\r\n",
      "catalogue                 1.0.0                    py38_1    defaults\r\n",
      "certifi                   2020.12.5        py38hecd8cb5_0    defaults\r\n",
      "cffi                      1.14.5           py38h2125817_0    defaults\r\n",
      "chardet                   4.0.0           py38hecd8cb5_1003    defaults\r\n",
      "click                     7.1.2              pyhd3eb1b0_0    defaults\r\n",
      "cryptography              3.3.1            py38hbcfaee0_1    defaults\r\n",
      "cycler                    0.10.0                   py38_0    defaults\r\n",
      "cymem                     2.0.5            py38h23ab428_0    defaults\r\n",
      "cython-blis               0.4.1            py38haf1e3a3_1    defaults\r\n",
      "dbus                      1.13.18              h18a8e69_0    defaults\r\n",
      "decorator                 4.4.2              pyhd3eb1b0_0    defaults\r\n",
      "defusedxml                0.6.0              pyhd3eb1b0_0    defaults\r\n",
      "en-core-web-sm            2.3.1                    pypi_0    pypi\r\n",
      "entrypoints               0.3                      py38_0    defaults\r\n",
      "expat                     2.2.10               hb1e8313_2    defaults\r\n",
      "freetype                  2.10.4               ha233b18_0    defaults\r\n",
      "gettext                   0.19.8.1             hb0f4f8b_2    defaults\r\n",
      "glib                      2.67.4               hdf23fa2_1    defaults\r\n",
      "gmp                       6.2.1                h23ab428_2    defaults\r\n",
      "icu                       58.2                 h0a44026_3    defaults\r\n",
      "idna                      2.10               pyhd3eb1b0_0    defaults\r\n",
      "importlib-metadata        2.0.0                      py_1    defaults\r\n",
      "importlib_metadata        2.0.0                         1    defaults\r\n",
      "intel-openmp              2019.4                      233    defaults\r\n",
      "ipykernel                 5.3.4            py38h5ca1d4c_0    defaults\r\n",
      "ipython                   7.21.0           py38h01d92e1_0    defaults\r\n",
      "ipython_genutils          0.2.0              pyhd3eb1b0_1    defaults\r\n",
      "ipywidgets                7.6.3              pyhd3eb1b0_1    defaults\r\n",
      "jedi                      0.17.2           py38hecd8cb5_1    defaults\r\n",
      "jinja2                    2.11.3             pyhd3eb1b0_0    defaults\r\n",
      "joblib                    1.0.1              pyhd3eb1b0_0    defaults\r\n",
      "jpeg                      9b                   he5867d9_2    defaults\r\n",
      "jsonschema                3.0.2                    py38_0    defaults\r\n",
      "jupyter                   1.0.0                    py38_7    defaults\r\n",
      "jupyter_client            6.1.7                      py_0    defaults\r\n",
      "jupyter_console           6.2.0                      py_0    defaults\r\n",
      "jupyter_contrib_core      0.3.3                      py_2    conda-forge\r\n",
      "jupyter_contrib_nbextensions 0.5.1            py38h32f6830_1    conda-forge\r\n",
      "jupyter_core              4.7.1            py38hecd8cb5_0    defaults\r\n",
      "jupyter_highlight_selected_word 0.2.0           py38h50d1736_1002    conda-forge\r\n",
      "jupyter_latex_envs        1.4.6           pyhd8ed1ab_1002    conda-forge\r\n",
      "jupyter_nbextensions_configurator 0.4.1            py38h50d1736_2    conda-forge\r\n",
      "jupyterlab_pygments       0.1.2                      py_0    defaults\r\n",
      "jupyterlab_widgets        1.0.0              pyhd3eb1b0_1    defaults\r\n",
      "kiwisolver                1.3.1            py38h23ab428_0    defaults\r\n",
      "lcms2                     2.11                 h92f6f08_0    defaults\r\n",
      "libcxx                    10.0.0                        1    defaults\r\n",
      "libedit                   3.1.20191231         h1de35cc_1    defaults\r\n",
      "libffi                    3.3                  hb1e8313_2    defaults\r\n",
      "libgfortran               3.0.1                h93005f0_2    defaults\r\n",
      "libiconv                  1.16                 h1de35cc_0    defaults\r\n",
      "libpng                    1.6.37               ha441bb4_0    defaults\r\n",
      "libsodium                 1.0.18               h1de35cc_0    defaults\r\n",
      "libtiff                   4.1.0                hcb84e12_1    defaults\r\n",
      "libxml2                   2.9.10               h7cdb67c_3    defaults\r\n",
      "libxslt                   1.1.34               h83b36ba_0    defaults\r\n",
      "llvm-openmp               10.0.0               h28b9765_0    defaults\r\n",
      "lxml                      4.6.2            py38h26b266a_0    defaults\r\n",
      "lz4-c                     1.9.3                h23ab428_0    defaults\r\n",
      "markupsafe                1.1.1            py38h1de35cc_1    defaults\r\n",
      "matplotlib                3.3.4            py38hecd8cb5_0    defaults\r\n",
      "matplotlib-base           3.3.4            py38h8b3ea08_0    defaults\r\n",
      "mistune                   0.8.4           py38h1de35cc_1001    defaults\r\n",
      "mkl                       2019.4                      233    defaults\r\n",
      "mkl-service               2.3.0            py38h9ed2024_0    defaults\r\n",
      "mkl_fft                   1.3.0            py38ha059aab_0    defaults\r\n",
      "mkl_random                1.1.1            py38h959d312_0    defaults\r\n",
      "murmurhash                1.0.5            py38h23ab428_0    defaults\r\n",
      "nbclient                  0.5.3              pyhd3eb1b0_0    defaults\r\n",
      "nbconvert                 6.0.7                    py38_0    defaults\r\n",
      "nbformat                  5.1.2              pyhd3eb1b0_1    defaults\r\n",
      "ncurses                   6.2                  h0a44026_1    defaults\r\n",
      "nest-asyncio              1.5.1              pyhd3eb1b0_0    defaults\r\n",
      "nltk                      3.5                        py_0    defaults\r",
      "\r\n",
      "notebook                  6.2.0            py38hecd8cb5_0    defaults\r\n",
      "numpy                     1.19.2           py38h456fd55_0    defaults\r\n",
      "numpy-base                1.19.2           py38hcfb5961_0    defaults\r\n",
      "olefile                   0.46                       py_0    defaults\r\n",
      "openssl                   1.1.1                h1de35cc_0    defaults\r\n",
      "packaging                 20.9               pyhd3eb1b0_0    defaults\r\n",
      "pandas                    1.2.3            py38hb2f4e1b_0    defaults\r\n",
      "pandoc                    2.11                 h0dc7051_0    defaults\r\n",
      "pandocfilters             1.4.3            py38hecd8cb5_1    defaults\r\n",
      "parso                     0.7.0                      py_0    defaults\r\n",
      "pcre                      8.44                 hb1e8313_0    defaults\r\n",
      "pdfminer-six              20200517                 pypi_0    pypi\r\n",
      "pdfplumber                0.5.24                   pypi_0    pypi\r\n",
      "pexpect                   4.8.0              pyhd3eb1b0_3    defaults\r\n",
      "pickleshare               0.7.5           pyhd3eb1b0_1003    defaults\r\n",
      "pillow                    8.1.1            py38h5270095_0    defaults\r\n",
      "pip                       21.0.1           py38hecd8cb5_0    defaults\r\n",
      "plac                      1.1.0                    py38_1    defaults\r\n",
      "preshed                   3.0.2            py38h23ab428_1    defaults\r\n",
      "prometheus_client         0.9.0              pyhd3eb1b0_0    defaults\r\n",
      "prompt-toolkit            3.0.8                      py_0    defaults\r\n",
      "prompt_toolkit            3.0.8                         0    defaults\r\n",
      "ptyprocess                0.7.0              pyhd3eb1b0_2    defaults\r\n",
      "pycparser                 2.20                       py_2    defaults\r\n",
      "pycryptodome              3.10.1           py38h659dea6_0    defaults\r\n",
      "pygments                  2.8.0              pyhd3eb1b0_0    defaults\r\n",
      "pyopenssl                 20.0.1             pyhd3eb1b0_1    defaults\r\n",
      "pyparsing                 2.4.7              pyhd3eb1b0_0    defaults\r\n",
      "pypdf2                    1.26.0                     py_2    conda-forge\r\n",
      "pyqt                      5.9.2            py38h655552a_2    defaults\r\n",
      "pyrsistent                0.17.3           py38haf1e3a3_0    defaults\r\n",
      "pysocks                   1.7.1                    py38_1    defaults\r\n",
      "python                    3.8.8                h88f2d9e_4    defaults\r\n",
      "python-dateutil           2.8.1              pyhd3eb1b0_0    defaults\r\n",
      "python_abi                3.8                      1_cp38    conda-forge\r\n",
      "pytz                      2021.1             pyhd3eb1b0_0    defaults\r\n",
      "pyyaml                    5.4.1            py38h9ed2024_1    defaults\r\n",
      "pyzmq                     20.0.0           py38h23ab428_1    defaults\r\n",
      "qt                        5.9.7                h468cd18_1    defaults\r\n",
      "qtconsole                 5.0.2              pyhd3eb1b0_0    defaults\r\n",
      "qtpy                      1.9.0                      py_0    defaults\r\n",
      "readline                  8.1                  h9ed2024_0    defaults\r\n",
      "regex                     2020.11.13       py38h9ed2024_0    defaults\r\n",
      "requests                  2.25.1             pyhd3eb1b0_0    defaults\r\n",
      "scikit-learn              0.24.1           py38hb2f4e1b_0    defaults\r\n",
      "scipy                     1.6.1            py38h2515648_0    defaults\r\n",
      "send2trash                1.5.0              pyhd3eb1b0_1    defaults\r\n",
      "setuptools                52.0.0           py38hecd8cb5_0    defaults\r\n",
      "sip                       4.19.8           py38h0a44026_0    defaults\r\n",
      "six                       1.15.0           py38hecd8cb5_0    defaults\r\n",
      "sortedcontainers          2.3.0              pyhd3eb1b0_0    defaults\r\n",
      "soupsieve                 2.2                pyhd3eb1b0_0    defaults\r\n",
      "spacy                     2.3.5            py38hf7b0b51_0    defaults\r\n",
      "sqlite                    3.33.0               hffcf06c_0    defaults\r\n",
      "srsly                     1.0.5            py38h23ab428_0    defaults\r\n",
      "terminado                 0.9.2            py38hecd8cb5_0    defaults\r\n",
      "testpath                  0.4.4              pyhd3eb1b0_0    defaults\r\n",
      "thinc                     7.4.1            py38h879752b_0    defaults\r\n",
      "threadpoolctl             2.1.0              pyh5ca1d4c_0    defaults\r\n",
      "tk                        8.6.10               hb0a8c7a_0    defaults\r\n",
      "tornado                   6.1              py38h9ed2024_0    defaults\r\n",
      "tqdm                      4.56.0             pyhd3eb1b0_0    defaults\r\n",
      "traitlets                 5.0.5              pyhd3eb1b0_0    defaults\r\n",
      "urllib3                   1.26.3             pyhd3eb1b0_0    defaults\r\n",
      "wand                      0.6.3                    pypi_0    pypi\r\n",
      "wasabi                    0.8.2              pyhd3eb1b0_0    defaults\r\n",
      "wcwidth                   0.2.5                      py_0    defaults\r\n",
      "webencodings              0.5.1                    py38_1    defaults\r\n",
      "wheel                     0.36.2             pyhd3eb1b0_0    defaults\r\n",
      "widgetsnbextension        3.5.1                    py38_0    defaults\r\n",
      "xz                        5.2.5                h1de35cc_0    defaults\r\n",
      "yaml                      0.2.5                haf1e3a3_0    defaults\r\n",
      "zeromq                    4.3.3                hb1e8313_3    defaults\r\n",
      "zipp                      3.4.0              pyhd3eb1b0_0    defaults\r\n",
      "zlib                      1.2.11               h1de35cc_3    defaults\r\n",
      "zstd                      1.4.5                h41d2c2f_0    defaults\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "neutral-stocks",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/bradgreenwald/projects/manning/Author_ID_Live_Project'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpreted-rough",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "interesting-crown",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from pprint import pprint as pp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greater-password",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "associate-distributor",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%config Completer.use_jedi=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-centre",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Helper classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "incredible-border",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dirp = lambda x: [d for d in dir(x) if not d.startswith('_')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weekly-lodge",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Where is the tutorial?  \n",
    "> Supposedly there is a `doc` dir under sklearn module, but I can't find it.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pediatric-voltage",
   "metadata": {},
   "source": [
    "# The Main Tutorial\n",
    "Working through **sklearn tutorial** as intro (and recollection to my earlier by interrupted work on NLP) to the Manning Live Project Authorship ID with Text Mining and ML  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-distance",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Load data and create dataset\n",
    "- Data will be subset of sklearn dataset called 20 Newsgroups.  The subset is specified by four (4) particular categories within that dataset.  According to sklearn, \"To the best of our knowledge, it was originally collected by Ken Lang, probably for his paper “Newsweeder: Learning to filter netnews,” though he does not explicitly mention this collection. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering.\"  \n",
    "\n",
    "The process is to use the built-in dataset loader for 20 newsgroups from scikit-learn. Alternatively, it is possible to download the dataset manually from the website and use the sklearn.datasets.load_files function by pointing it to the 20news-bydate-train sub-folder of the uncompressed archive folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "simple-failing",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# setup limited list of categories to create smaller dataset to be faster\n",
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acute-invention",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "danish-serve",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# subset will be records from each of the four categories, shuffled\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "superior-sleep",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DESCR', 'data', 'filenames', 'target', 'target_names']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# twenty_train is a sklearn \"dataset\" which is like a dict with the following keys\n",
    "dir(twenty_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "small-rebel",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the subset of categories\n",
    "twenty_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "auburn-shaft",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2257"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(twenty_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "innovative-agreement",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2257"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(twenty_train.filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "broadband-championship",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: sd345@city.ac.uk (Michael Collier)\n",
      "Subject: Converting images to HP LaserJet III?\n",
      "Nntp-Posting-Host: hampton\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(twenty_train.data[0].split('\\n')[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "norwegian-flashing",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 3, 3, 3, 3, 3, 2, 2, 2])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labels encoded as ints\n",
    "twenty_train.target[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-robinson",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## BG practice - refamiliarize myself by building a BOW vectorizer and related functions from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "computational-excess",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Functions developed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "approximate-least",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def preprocess(doc):\n",
    "    \"\"\"Create a bag of words from doc, which is a single string\"\"\"\n",
    "    # replace linebreaks with spaces (not empty string because sometimes line break separates words)\n",
    "    temp = doc.replace('\\n', ' ')\n",
    "    \n",
    "    # split on white space, which may leave many empty strings needing to be removed from the list\n",
    "    temp = temp.split(' ')\n",
    "    temp = [tok for tok in temp if tok != '']\n",
    "    \n",
    "    # remove punct at start or end of words\n",
    "    punct1 = '.:,?)(]['\n",
    "    temp = mod_punct(temp, punct1)\n",
    "    \n",
    "    # Convert to lowercase (could just do initial caps now that punct is removed from beginning of words)\n",
    "    temp = [word.lower() for word in temp]\n",
    "    \n",
    "    return temp\n",
    "\n",
    "\n",
    "def create_wordmap_to_vectorize(words):\n",
    "    \"\"\"Create a dict and reverse-dict for mapping words to int\"\"\"\n",
    "    word_set = set(words)   # remove dupes\n",
    "    word_int_map = dict()\n",
    "    int_word_map = dict()\n",
    "    \n",
    "    # create dict and rev dict\n",
    "    for idx, word in enumerate(word_set):\n",
    "        word_int_map[idx] = word\n",
    "        int_word_map[word] = idx\n",
    "        \n",
    "    return word_int_map, int_word_map\n",
    "\n",
    "\n",
    "def vectorize_wordlist(wordmap, wordlist):\n",
    "    \"\"\"Create and return a word vector which has frequencies of words in wordlist at the positional index. \"\"\"\n",
    "    word_vector = [0] * len(wordmap)\n",
    "    for word in wordlist:        \n",
    "        word_vector[wordmap[word]] += 1\n",
    "    return word_vector\n",
    "\n",
    "\n",
    "def mod_punct(word_list, punct):\n",
    "    \"\"\"Modify words in word list by removing punctuation at start or end of word\"\"\"\n",
    "    new_words = []\n",
    "    for word in word_list:\n",
    "        if word[-1] in punct:\n",
    "            if word[0] in punct:\n",
    "                new_words.append(word[1:-1])\n",
    "            else:\n",
    "                new_words.append(word[:-1])\n",
    "        elif word[0] in punct:\n",
    "            new_words.append(word[1:])\n",
    "\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-thong",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Test of interim functioning of preprocessing, including removing punc, splitting on whitespace, lowercaseing, creating word map, then creating a word vector.   Functions developed below are hoisted and then tested immediately above\n",
    "- then retested with 2d data item and it seems to work.  its quite basic, and as noted below there are many limitations, but its a very good start, I think.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "retired-mention",
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: ani@ms.uky.edu (Aniruddha B. Deglurkar)\n",
      "Subject: help: Splitting a trimming region along a mesh \n",
      "Organization: University Of Kentucky, Dept. of Math Sciences\n",
      "Lines: 28\n",
      "\n",
      "\n",
      "\n",
      "\tHi,\n",
      "\n",
      "\tI have a problem, I hope some of the 'gurus' can help me solve.\n",
      "\n",
      "\tBackground of the problem:\n",
      "\tI have a rectangular mesh in the uv domain, i.e  the mesh is a \n",
      "\tmapping of a 3d Bezier patch into 2d. The area in this domain\n",
      "\twhich is inside a trimming loop had to be rendered. The trimming\n",
      "\tloop is a set of 2d Bezier curve segments.\n",
      "\tFor the sake of notation: the mesh is made up of cells.\n",
      "\n",
      "\tMy problem is this :\n",
      "\tThe trimming area has to be split up into individual smaller\n",
      "\tcells bounded by the trimming curve segments. If a cell\n",
      "\tis wholly inside the area...then it is output as a whole ,\n",
      "\telse it is trivially rejected. \n",
      "\n",
      "\tDoes any body know how thiss can be done, or is there any algo. \n",
      "\tsomewhere for doing this.\n",
      "\n",
      "\tAny help would be appreciated.\n",
      "\n",
      "\tThanks, \n",
      "\tAni.\n",
      "-- \n",
      "To get irritated is human, to stay cool, divine.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc2 = twenty_train.data[1]\n",
    "print(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "intimate-tunnel",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pp_doc = preprocess(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "isolated-credit",
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(181,\n",
       " ['from',\n",
       "  'ani@ms.uky.edu',\n",
       "  'aniruddha',\n",
       "  'b',\n",
       "  'deglurkar',\n",
       "  'subject',\n",
       "  'help',\n",
       "  'splitting',\n",
       "  'a',\n",
       "  'trimming',\n",
       "  'region',\n",
       "  'along',\n",
       "  'a',\n",
       "  'mesh',\n",
       "  'organization',\n",
       "  'university',\n",
       "  'of',\n",
       "  'kentucky',\n",
       "  'dept',\n",
       "  'of',\n",
       "  'math',\n",
       "  'sciences',\n",
       "  'lines',\n",
       "  '28',\n",
       "  '\\thi',\n",
       "  '\\ti',\n",
       "  'have',\n",
       "  'a',\n",
       "  'problem',\n",
       "  'i',\n",
       "  'hope',\n",
       "  'some',\n",
       "  'of',\n",
       "  'the',\n",
       "  \"'gurus'\",\n",
       "  'can',\n",
       "  'help',\n",
       "  'me',\n",
       "  'solve',\n",
       "  '\\tbackground',\n",
       "  'of',\n",
       "  'the',\n",
       "  'problem',\n",
       "  '\\ti',\n",
       "  'have',\n",
       "  'a',\n",
       "  'rectangular',\n",
       "  'mesh',\n",
       "  'in',\n",
       "  'the',\n",
       "  'uv',\n",
       "  'domain',\n",
       "  'i.e',\n",
       "  'the',\n",
       "  'mesh',\n",
       "  'is',\n",
       "  'a',\n",
       "  '\\tmapping',\n",
       "  'of',\n",
       "  'a',\n",
       "  '3d',\n",
       "  'bezier',\n",
       "  'patch',\n",
       "  'into',\n",
       "  '2d',\n",
       "  'the',\n",
       "  'area',\n",
       "  'in',\n",
       "  'this',\n",
       "  'domain',\n",
       "  '\\twhich',\n",
       "  'is',\n",
       "  'inside',\n",
       "  'a',\n",
       "  'trimming',\n",
       "  'loop',\n",
       "  'had',\n",
       "  'to',\n",
       "  'be',\n",
       "  'rendered',\n",
       "  'the',\n",
       "  'trimming',\n",
       "  '\\tloop',\n",
       "  'is',\n",
       "  'a',\n",
       "  'set',\n",
       "  'of',\n",
       "  '2d',\n",
       "  'bezier',\n",
       "  'curve',\n",
       "  'segments',\n",
       "  '\\tfor',\n",
       "  'the',\n",
       "  'sake',\n",
       "  'of',\n",
       "  'notation',\n",
       "  'the',\n",
       "  'mesh',\n",
       "  'is',\n",
       "  'made',\n",
       "  'up',\n",
       "  'of',\n",
       "  'cells',\n",
       "  '\\tmy',\n",
       "  'problem',\n",
       "  'is',\n",
       "  'this',\n",
       "  '',\n",
       "  '\\tthe',\n",
       "  'trimming',\n",
       "  'area',\n",
       "  'has',\n",
       "  'to',\n",
       "  'be',\n",
       "  'split',\n",
       "  'up',\n",
       "  'into',\n",
       "  'individual',\n",
       "  'smaller',\n",
       "  '\\tcells',\n",
       "  'bounded',\n",
       "  'by',\n",
       "  'the',\n",
       "  'trimming',\n",
       "  'curve',\n",
       "  'segments',\n",
       "  'if',\n",
       "  'a',\n",
       "  'cell',\n",
       "  '\\tis',\n",
       "  'wholly',\n",
       "  'inside',\n",
       "  'the',\n",
       "  'area...then',\n",
       "  'it',\n",
       "  'is',\n",
       "  'output',\n",
       "  'as',\n",
       "  'a',\n",
       "  'whole',\n",
       "  '',\n",
       "  '\\telse',\n",
       "  'it',\n",
       "  'is',\n",
       "  'trivially',\n",
       "  'rejected',\n",
       "  '\\tdoes',\n",
       "  'any',\n",
       "  'body',\n",
       "  'know',\n",
       "  'how',\n",
       "  'thiss',\n",
       "  'can',\n",
       "  'be',\n",
       "  'done',\n",
       "  'or',\n",
       "  'is',\n",
       "  'there',\n",
       "  'any',\n",
       "  'algo',\n",
       "  '\\tsomewhere',\n",
       "  'for',\n",
       "  'doing',\n",
       "  'this',\n",
       "  '\\tany',\n",
       "  'help',\n",
       "  'would',\n",
       "  'be',\n",
       "  'appreciated',\n",
       "  '\\tthanks',\n",
       "  '\\tani',\n",
       "  '--',\n",
       "  'to',\n",
       "  'get',\n",
       "  'irritated',\n",
       "  'is',\n",
       "  'human',\n",
       "  'to',\n",
       "  'stay',\n",
       "  'cool',\n",
       "  'divine'])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pp_doc), pp_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "stupid-juvenile",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "int_idx, word_idx = create_wordmap_to_vectorize(pp_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "turkish-spoke",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(181, 113, 181)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_vec = vectorize_wordlist(word_idx, pp_doc)\n",
    "len(pp_doc), len(w_vec), sum(w_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-rating",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "responsible-hampton",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The most intuitive way to do so is to use a bags of words representation:\n",
    "\n",
    "- Assign a fixed integer id to each word occurring in any document of the training set (for instance by building a dictionary from words to integer indices).\n",
    "- For each document #i, count the number of occurrences of each word w and store it in X[i, j] as the value of feature #j where j is the index of word w in the dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "express-acrobat",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-drilling",
   "metadata": {
    "hidden": true
   },
   "source": [
    "BG - lets start with toy example just to have the simple idea muscled out  \n",
    "1. For a given document, need a list of Each Word in that document.  How do we do that? We could break on spaces?  Remove punctuation?  \n",
    "    - Lets see what we get by splitting on white space; try first doc in dataset\n",
    "        1. First problem is formatting codes, e.g. line breaks '\\n'.  We shouldn't remove them, because they are word breaks, so prob replace with space, and then split on white space\n",
    "        2. Second problem is removing multiple white spaces, which we can do with list comprehension after split, by removing all empty strings.  \n",
    "        3. Removing punctuation is difficult if dealing with things like email addresses, or urls.  Need to identify first.  Or maybe for periods only sub whitespace if there is it least one white space after?  Another idea is to remove ONLY AFTER word list is created, and then only if it is at start or end.  \n",
    "        4. re Punctuation, also may indicate presence of something like a fax number, address, etc.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "still-width",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### A mini-function with preprocessing and tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "rising-plate",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def preprocess(doc):\n",
    "    \"\"\"Create a bag of words from doc, which is a single string\"\"\"\n",
    "    # replace linebreaks with spaces (not empty string because sometimes line break separates words)\n",
    "    temp = doc.replace('\\n', ' ')\n",
    "    \n",
    "    # replace punctuation with white space; eventually consider regex, for now use a list\n",
    "    punct = ':/?'\n",
    "    \n",
    "    # split on white space, which may leave many empty strings needing to be removed from the list\n",
    "    temp = temp.split(' ')\n",
    "    temp = [tok for tok in temp if tok != '']   \n",
    "    \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "amino-profit",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From: sd345@city.ac.uk (Michael Collier)\\nSubject: Converting images to HP LaserJet III?\\nNntp-Posting-Host: hampton\\nOrganization: The City University\\nLines: 14\\n\\nDoes anyone know of a good way (standard PC application/PD utility) to\\nconvert tif/img/tga files into LaserJet III format.  We would also like to\\ndo the same, converting to HPGL (HP plotter) files.\\n\\nPlease email any response.\\n\\nIs this the correct group?\\n\\nThanks in advance.  Michael.\\n-- \\nMichael Collier (Programmer)                 The Computer Unit,\\nEmail: M.P.Collier@uk.ac.city                The City University,\\nTel: 071 477-8000 x3769                      London,\\nFax: 071 477-8565                            EC1V 0HB.\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1 = twenty_train.data[0]\n",
    "doc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "amended-jewelry",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: sd345@city.ac.uk (Michael Collier)\n",
      "Subject: Converting images to HP LaserJet III?\n",
      "Nntp-Posting-Host: hampton\n",
      "Organization: The City University\n",
      "Lines: 14\n",
      "\n",
      "Does anyone know of a good way (standard PC application/PD utility) to\n",
      "convert tif/img/tga files into LaserJet III format.  We would also like to\n",
      "do the same, converting to HPGL (HP plotter) files.\n",
      "\n",
      "Please email any response.\n",
      "\n",
      "Is this the correct group?\n",
      "\n",
      "Thanks in advance.  Michael.\n",
      "-- \n",
      "Michael Collier (Programmer)                 The Computer Unit,\n",
      "Email: M.P.Collier@uk.ac.city                The City University,\n",
      "Tel: 071 477-8000 x3769                      London,\n",
      "Fax: 071 477-8565                            EC1V 0HB.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "certain-number",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# splitting on spaces with no preprocessing gives 154 words\n",
    "w1 = doc1.split(' ')\n",
    "len(w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "developed-housing",
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From:', 'sd345@city.ac.uk', '(Michael', 'Collier)\\nSubject:', 'Converting', 'images', 'to', 'HP', 'LaserJet', 'III?\\nNntp-Posting-Host:', 'hampton\\nOrganization:', 'The', 'City', 'University\\nLines:', '14\\n\\nDoes', 'anyone', 'know', 'of', 'a', 'good', 'way', '(standard', 'PC', 'application/PD', 'utility)', 'to\\nconvert', 'tif/img/tga', 'files', 'into', 'LaserJet', 'III', 'format.', '', 'We', 'would', 'also', 'like', 'to\\ndo', 'the', 'same,', 'converting', 'to', 'HPGL', '(HP', 'plotter)', 'files.\\n\\nPlease', 'email', 'any', 'response.\\n\\nIs', 'this', 'the', 'correct', 'group?\\n\\nThanks', 'in', 'advance.', '', 'Michael.\\n--', '\\nMichael', 'Collier', '(Programmer)', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'The', 'Computer', 'Unit,\\nEmail:', 'M.P.Collier@uk.ac.city', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'The', 'City', 'University,\\nTel:', '071', '477-8000', 'x3769', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'London,\\nFax:', '071', '477-8565', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'EC1V', '0HB.\\n']\n"
     ]
    }
   ],
   "source": [
    "print(w1, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accompanied-culture",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What are all these multiple spaces??  Some of it is formatting in combination with white space.  Maybe we can remove all white space greater than 1.  Well, once we have the list of tokens, we can just remove any of them that are empty string.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "smart-trainer",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use list comprehension to create new list with no empty strings\n",
    "w1a = [w for w in w1 if w != '']\n",
    "len(w1a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structural-recognition",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So that removed over 1/2 of the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "received-dutch",
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['From:',\n",
       " 'sd345@city.ac.uk',\n",
       " '(Michael',\n",
       " 'Collier)\\nSubject:',\n",
       " 'Converting',\n",
       " 'images',\n",
       " 'to',\n",
       " 'HP',\n",
       " 'LaserJet',\n",
       " 'III?\\nNntp-Posting-Host:',\n",
       " 'hampton\\nOrganization:',\n",
       " 'The',\n",
       " 'City',\n",
       " 'University\\nLines:',\n",
       " '14\\n\\nDoes',\n",
       " 'anyone',\n",
       " 'know',\n",
       " 'of',\n",
       " 'a',\n",
       " 'good',\n",
       " 'way',\n",
       " '(standard',\n",
       " 'PC',\n",
       " 'application/PD',\n",
       " 'utility)',\n",
       " 'to\\nconvert',\n",
       " 'tif/img/tga',\n",
       " 'files',\n",
       " 'into',\n",
       " 'LaserJet',\n",
       " 'III',\n",
       " 'format.',\n",
       " 'We',\n",
       " 'would',\n",
       " 'also',\n",
       " 'like',\n",
       " 'to\\ndo',\n",
       " 'the',\n",
       " 'same,',\n",
       " 'converting',\n",
       " 'to',\n",
       " 'HPGL',\n",
       " '(HP',\n",
       " 'plotter)',\n",
       " 'files.\\n\\nPlease',\n",
       " 'email',\n",
       " 'any',\n",
       " 'response.\\n\\nIs',\n",
       " 'this',\n",
       " 'the',\n",
       " 'correct',\n",
       " 'group?\\n\\nThanks',\n",
       " 'in',\n",
       " 'advance.',\n",
       " 'Michael.\\n--',\n",
       " '\\nMichael',\n",
       " 'Collier',\n",
       " '(Programmer)',\n",
       " 'The',\n",
       " 'Computer',\n",
       " 'Unit,\\nEmail:',\n",
       " 'M.P.Collier@uk.ac.city',\n",
       " 'The',\n",
       " 'City',\n",
       " 'University,\\nTel:',\n",
       " '071',\n",
       " '477-8000',\n",
       " 'x3769',\n",
       " 'London,\\nFax:',\n",
       " '071',\n",
       " '477-8565',\n",
       " 'EC1V',\n",
       " '0HB.\\n']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "injured-process",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# replace linebreaks\n",
    "w2 = doc1.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "suitable-eligibility",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "174"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use string split to create word list\n",
    "w2 = w2.split(' ')\n",
    "len(w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geological-sauce",
   "metadata": {
    "hidden": true
   },
   "source": [
    "IDEA:  we can incrementally build a function by adding each of these steps as we work them out\n",
    "- See above for implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "gentle-canada",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove empty strings\n",
    "w2a = [w for w in w2 if w != '']\n",
    "len(w2a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coastal-creativity",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "unauthorized-budapest",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "punct1 = '.:,?)]['"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "urban-gregory",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#  to retain some phrases that include punct chars\n",
    "def mod_punct(word_list, punct):\n",
    "    \"\"\"Modify words in word list by removing punctuation at start or end of word\"\"\"\n",
    "    new_words = []\n",
    "    for word in word_list:\n",
    "        if word[-1] in punct1:\n",
    "            if word[0] in punct1:\n",
    "                new_words.append(word[1:-1])\n",
    "            else:\n",
    "                new_words.append(word[:-1])\n",
    "        elif word[0] in punct1:\n",
    "            new_words.append(word[1:])\n",
    "\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "supported-companion",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['From',\n",
       " 'sd345@city.ac.uk',\n",
       " '(Michael',\n",
       " 'Collier',\n",
       " 'Subject',\n",
       " 'Converting',\n",
       " 'images',\n",
       " 'to',\n",
       " 'HP',\n",
       " 'LaserJet',\n",
       " 'III',\n",
       " 'Nntp-Posting-Host',\n",
       " 'hampton',\n",
       " 'Organization',\n",
       " 'The',\n",
       " 'City',\n",
       " 'University',\n",
       " 'Lines',\n",
       " '14',\n",
       " 'Does',\n",
       " 'anyone',\n",
       " 'know',\n",
       " 'of',\n",
       " 'a',\n",
       " 'good',\n",
       " 'way',\n",
       " '(standard',\n",
       " 'PC',\n",
       " 'application/PD',\n",
       " 'utility',\n",
       " 'to',\n",
       " 'convert',\n",
       " 'tif/img/tga',\n",
       " 'files',\n",
       " 'into',\n",
       " 'LaserJet',\n",
       " 'III',\n",
       " 'format',\n",
       " 'We',\n",
       " 'would',\n",
       " 'also',\n",
       " 'like',\n",
       " 'to',\n",
       " 'do',\n",
       " 'the',\n",
       " 'same',\n",
       " 'converting',\n",
       " 'to',\n",
       " 'HPGL',\n",
       " '(HP',\n",
       " 'plotter',\n",
       " 'files',\n",
       " 'Please',\n",
       " 'email',\n",
       " 'any',\n",
       " 'response',\n",
       " 'Is',\n",
       " 'this',\n",
       " 'the',\n",
       " 'correct',\n",
       " 'group',\n",
       " 'Thanks',\n",
       " 'in',\n",
       " 'advance',\n",
       " 'Michael',\n",
       " '--',\n",
       " 'Michael',\n",
       " 'Collier',\n",
       " '(Programmer',\n",
       " 'The',\n",
       " 'Computer',\n",
       " 'Unit',\n",
       " 'Email',\n",
       " 'M.P.Collier@uk.ac.city',\n",
       " 'The',\n",
       " 'City',\n",
       " 'University',\n",
       " 'Tel',\n",
       " '071',\n",
       " '477-8000',\n",
       " 'x3769',\n",
       " 'London',\n",
       " 'Fax',\n",
       " '071',\n",
       " '477-8565',\n",
       " 'EC1V',\n",
       " '0HB']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_punct(w2a, punct1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "caring-reduction",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['From:',\n",
       " 'sd345@city.ac.uk',\n",
       " '(Michael',\n",
       " 'Collier)',\n",
       " 'Subject:',\n",
       " 'Converting',\n",
       " 'images',\n",
       " 'to',\n",
       " 'HP',\n",
       " 'LaserJet',\n",
       " 'III?',\n",
       " 'Nntp-Posting-Host:',\n",
       " 'hampton',\n",
       " 'Organization:',\n",
       " 'The',\n",
       " 'City',\n",
       " 'University',\n",
       " 'Lines:',\n",
       " '14',\n",
       " 'Does',\n",
       " 'anyone',\n",
       " 'know',\n",
       " 'of',\n",
       " 'a',\n",
       " 'good',\n",
       " 'way',\n",
       " '(standard',\n",
       " 'PC',\n",
       " 'application/PD',\n",
       " 'utility)',\n",
       " 'to',\n",
       " 'convert',\n",
       " 'tif/img/tga',\n",
       " 'files',\n",
       " 'into',\n",
       " 'LaserJet',\n",
       " 'III',\n",
       " 'format.',\n",
       " 'We',\n",
       " 'would',\n",
       " 'also',\n",
       " 'like',\n",
       " 'to',\n",
       " 'do',\n",
       " 'the',\n",
       " 'same,',\n",
       " 'converting',\n",
       " 'to',\n",
       " 'HPGL',\n",
       " '(HP',\n",
       " 'plotter)',\n",
       " 'files.',\n",
       " 'Please',\n",
       " 'email',\n",
       " 'any',\n",
       " 'response.',\n",
       " 'Is',\n",
       " 'this',\n",
       " 'the',\n",
       " 'correct',\n",
       " 'group?',\n",
       " 'Thanks',\n",
       " 'in',\n",
       " 'advance.',\n",
       " 'Michael.',\n",
       " '--',\n",
       " 'Michael',\n",
       " 'Collier',\n",
       " '(Programmer)',\n",
       " 'The',\n",
       " 'Computer',\n",
       " 'Unit,',\n",
       " 'Email:',\n",
       " 'M.P.Collier@uk.ac.city',\n",
       " 'The',\n",
       " 'City',\n",
       " 'University,',\n",
       " 'Tel:',\n",
       " '071',\n",
       " '477-8000',\n",
       " 'x3769',\n",
       " 'London,',\n",
       " 'Fax:',\n",
       " '071',\n",
       " '477-8565',\n",
       " 'EC1V',\n",
       " '0HB.']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ethical-queens",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c = Counter(w2a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "european-mandate",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('to', 4),\n",
       " ('The', 3),\n",
       " ('LaserJet', 2),\n",
       " ('City', 2),\n",
       " ('the', 2),\n",
       " ('071', 2),\n",
       " ('From:', 1),\n",
       " ('sd345@city.ac.uk', 1),\n",
       " ('(Michael', 1),\n",
       " ('Collier)', 1)]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words1 = c.most_common(10)\n",
    "words1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increasing-cincinnati",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Definitely starting to look better\n",
    "- Still to do: remove punctuation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nonprofit-wonder",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### How to build an integer indexed dictionary of words that can be used to convert a bag-of-words to a bag of numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "enhanced-checklist",
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to', 'The', 'LaserJet', 'City', 'the']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build a dictionary\n",
    "word_list = [w[0] for w in words1]\n",
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "continuous-integration",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_wordmap_to_vectorize(words):\n",
    "    \"\"\"Create a dict and reverse-dict for mapping words to int\"\"\"\n",
    "    word_set = set(words)   # remove dupes\n",
    "    word_int_map = dict()\n",
    "    int_word_map = dict()\n",
    "    \n",
    "    # create dict and rev dict\n",
    "    for idx, word in enumerate(word_set):\n",
    "        word_int_map[idx] = word\n",
    "        int_word_map[word] = idx\n",
    "        \n",
    "    return word_int_map, int_word_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "dramatic-local",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "word_map, rev_word_map = create_wordmap_to_vectorize(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "cooked-forestry",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'to', 1: 'The', 2: 'LaserJet', 3: 'the', 4: 'City'}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hundred-amazon",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Maybe instead of a dict create just a list where the word is simply mapped to its index-pos in the list.  \n",
    "- Access time is prob the same because indexing a list positionally is O(1).\n",
    "Another issue is that these vectors will be long, as long as the entire list of words. Prob a dict representation using word-ints as keys and vals as frequencies will be faster, assuming that lack of entry means zero. \n",
    "Ultimately, may want the function to take in the document instead of a word list.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "julian-burner",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# apply wordmap to wordlist\n",
    "def vectorize_wordlist(wordmap, wordlist):\n",
    "    \"\"\"Create a word vector meaning a for each int index in wordmap if the word occurs we increment the vector\n",
    "    element at that position\"\"\"\n",
    "    word_vector = [0] * len(wordmap)\n",
    "    for word in wordlist:\n",
    "        word_vector[wordmap[word]] += 1\n",
    "    return word_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "israeli-routine",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_wordlist(rev_word_map, word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "incomplete-abortion",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87, 78)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2a), len(word_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "desirable-omega",
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try with wordlist w2a from above - nice a 1.78 vector. \n",
    "int_keys, word_keys = create_wordmap_to_vectorize(w2a)\n",
    "vectorize_wordlist(word_keys, w2a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-fabric",
   "metadata": {},
   "source": [
    "## Feature Extraction - back to tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "racial-johnston",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Tokenizing\n",
    "- including text preprocessing (as above, there are several things to deal with here such as formatting, punctuation, case, whitespace, etc; tokenizing; filtering stopwords.  For the tutorial we use CountVectorizer for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "suspected-cooperation",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "noble-differential",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# instantiate\n",
    "count_vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "joint-fifty",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 35788)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# learn vocab and return document-term matrix\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "about-homework",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We get a sparse matrix, in particular csr\n",
    "type(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "handy-growth",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4690"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is frequency in entire corpus\n",
    "count_vect.vocabulary_.get(u'algorithm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advance-inclusion",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Note on term frequencies as a feature\n",
    "Dealing with a corpus, need to account for longer v shorter documents, e.g. normalizing via tf-idf, so for longer documents dividing by the total number of words offsets for the potential increased frequency of a word.  \n",
    "\n",
    "Another idea is that a word common to more documents in a corpus provides less differentiation, to the extent that is an issue, we can divide by the number of documents that the term occurs in.  It is 'inverse' because the more documents, the less important in this regard.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appropriate-narrow",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "compliant-pastor",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 35788)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This builds on the previous step - we are using the counts from CountVectorizer as argument\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "corrected-constraint",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 35788)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-thunder",
   "metadata": {},
   "source": [
    "## Training a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-diameter",
   "metadata": {},
   "source": [
    "Recall from the Kadenze work on Naive Bayes, and rereading sklearn on Naive Bayes.  We get a model, for each potential class of the classifier, (here we have 4), which is derived from the feature frequencies of the data given the class.  To classify, we compute the probability/likelihood of the new event/observation for each of the classes, and whichever is higher, that class is predicted.  Also called Maximum Likelihood Prediction.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "female-regulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with Naive Bayes classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "simple-alpha",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257,)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "possible-catholic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'God is love' => soc.religion.christian\n",
      "'OpenGL on the GPU is fast' => comp.graphics\n"
     ]
    }
   ],
   "source": [
    "# Some toy prediction; use the trained count_vect and tfidf transformer\n",
    "docs_new = ['God is love', 'OpenGL on the GPU is fast']\n",
    "\n",
    "# these transformers convert the document to a token vector based on the transforms as trained by the training data\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)   #  Notice we use the counts as argument\n",
    "predicted = clf.predict(X_new_tfidf) # X_new_tfidf contains the new docs transformed into tfidf vectors\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, twenty_train.target_names[category]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mineral-footage",
   "metadata": {},
   "source": [
    "## Building a Pipeline\n",
    "- Put the steps together:  vectorizer => transformer => classifier\n",
    "- use Pipeline from sklearn.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "lasting-individual",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "computational-white",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()), \n",
    "    ('tfidf', TfidfTransformer()), \n",
    "    ('clf', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "considerable-victoria",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
       "                ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With this set up we can train the model with a single command\n",
    "text_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-vitamin",
   "metadata": {},
   "source": [
    "### Evaluate performance on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "starting-refund",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8348868175765646"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)\n",
    "docs_test = twenty_test.data\n",
    "predicted = text_clf.predict(docs_test)\n",
    "np.mean(predicted == twenty_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worthy-semester",
   "metadata": {},
   "source": [
    "### Try an SVM (Support Vector Machine) - sub into the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "sealed-pledge",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "respected-negotiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()), \n",
    "    ('tfidf', TfidfTransformer()), \n",
    "    ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                         alpha=1e-3, random_state=42,\n",
    "                         max_iter=5, tol=None)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "simplified-promotion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9101198402130493"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit(twenty_train.data, twenty_train.target)\n",
    "predicted = text_clf.predict(docs_test)\n",
    "np.mean(predicted == twenty_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "ancient-small",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "mexican-surname",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.95      0.80      0.87       319\n",
      "         comp.graphics       0.87      0.98      0.92       389\n",
      "               sci.med       0.94      0.89      0.91       396\n",
      "soc.religion.christian       0.90      0.95      0.93       398\n",
      "\n",
      "              accuracy                           0.91      1502\n",
      "             macro avg       0.91      0.91      0.91      1502\n",
      "          weighted avg       0.91      0.91      0.91      1502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(twenty_test.target, predicted, target_names=twenty_test.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "economic-indie",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[256,  11,  16,  36],\n",
       "       [  4, 380,   3,   2],\n",
       "       [  5,  35, 353,   3],\n",
       "       [  5,  11,   4, 378]])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(twenty_test.target, predicted )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-latitude",
   "metadata": {},
   "source": [
    "### Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "appropriate-suggestion",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "streaming-relationship",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'clf__alpha': (1e-2, 1e-3),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "unlimited-banks",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can be computationally expensive\n",
    "gs_clf = GridSearchCV(text_clf, parameters, cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "driven-scanning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply to portion of data\n",
    "gs_clf = gs_clf.fit(twenty_train.data[:400], twenty_train.target[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "aquatic-replica",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'soc.religion.christian'"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target_names[gs_clf.predict([docs_new[0]])[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premier-wagner",
   "metadata": {},
   "source": [
    "# First Exercise - Language Identification\n",
    "- Write a text classification pipeline using a custom preprocessor and CharNGramAnalyzer using data from Wikipedia articles as training set.\n",
    "- Evaluate the performance on some held out test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electrical-blond",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Build a language detector model\n",
    "\n",
    "The goal of this exercise is to train a linear classifier on text features\n",
    "that represent sequences of up to 3 consecutive characters so as to be\n",
    "recognize natural languages by using the frequencies of short character\n",
    "sequences as 'fingerprints'.\n",
    "\n",
    "\"\"\"\n",
    "# Author: Olivier Grisel <olivier.grisel@ensta.org>\n",
    "# License: Simplified BSD\n",
    "\n",
    "import sys\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "unexpected-immigration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data folder from how I imported the tutorial\n",
    "data_folder = './code/data/languages/paragraphs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "destroyed-africa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training data folder must be passed as first argument\n",
    "languages_data_folder = data_folder\n",
    "dataset = load_files(languages_data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "textile-vampire",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DESCR', 'data', 'filenames', 'target', 'target_names']"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another sklearn dataset\n",
    "dir(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "piano-layer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Alguns observadores afirmam que a Wikip\\xc3\\xa9dia \\xc3\\xa9 uma amea\\xc3\\xa7a econ\\xc3\\xb4mica para os editores de enciclop\\xc3\\xa9dias tradicionais, muitos dos quais podem ser incapazes de competir com um produto que \\xc3\\xa9 essencialmente livre. Nicholas Carr escreveu no ensaio \"A amoralidade da Web 2.0\", falando sobre a chamada \"Web 2.0\" como um todo: \"impl\\xc3\\xadcita nas vis\\xc3\\xb5es de \\xc3\\xaaxtase de Web 2.0 \\xc3\\xa9 a hegemonia do amador. Eu, pelo menos, n\\xc3\\xa3o posso imaginar nada mais assustador.\"[159]\\n'"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "drawn-unknown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset in training and test set:\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "    dataset.data, dataset.target, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automatic-characterization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: Build a vectorizer that splits strings into sequence of 1 to 3\n",
    "# characters instead of word tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polished-stable",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: Build a vectorizer / classifier pipeline using the previous analyzer\n",
    "# the pipeline instance should stored in a variable named clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-berry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: Fit the pipeline on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: Predict the outcome on the testing set in a variable named y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-hours",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the classification report\n",
    "print(metrics.classification_report(y_test, y_predicted,\n",
    "                                    target_names=dataset.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applied-antigua",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, y_predicted)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-librarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "#plt.matshow(cm, cmap=plt.cm.jet)\n",
    "#plt.show()\n",
    "\n",
    "# Predict the result on some short new sentences:\n",
    "sentences = [\n",
    "    'This is a language detection test.',\n",
    "    'Ceci est un test de d\\xe9tection de la langue.',\n",
    "    'Dies ist ein Test, um die Sprache zu erkennen.',\n",
    "]\n",
    "predicted = clf.predict(sentences)\n",
    "\n",
    "for s, p in zip(sentences, predicted):\n",
    "    print('The language of \"%s\" is \"%s\"' % (s, dataset.target_names[p]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "light-wings",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-morris",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "238px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
